{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import codecs\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>0. Gathering the Data</h2>\n",
    "<p>In this preliminary section, we will gather all of the cuneiform sign transliterations from the JSON files in our dataset. Then we will consolidate them into a data frame and match each sign value with its sign name</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Do we need to discuss the basics of cuneiform transliteration or is it assumed that our audience is familiar with it?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>0.1: OGSL</h3>\n",
    "<p>Now, we will load a map from sign value to sign name to use on the signs in our texts. The OGSL is... (website...)</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_ogsl = codecs.open('ogsl-sl.json','r','utf-8')\n",
    "ogsl = json.load(file_ogsl)\n",
    "sign_index = ogsl['index']\n",
    "sign_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>0.2: Collect the Text Signs</h3>\n",
    "<p>The following code parses the JSON files of the ORACC texts and collects each sign transliteration. Since different signs have different types of reading, they are rendered differently in the JSON file and we must take care to recognize each sign reading type in the JSON file</p>\n",
    "The types of signs and their representation in the JSON Files:\n",
    "<ol>\n",
    "    <li>Syllable - The reading of a sign as a syllable is rendered with a 'v' key</li>\n",
    "    <li>Logogram - The reading of a sign as a logogram, i.e. one represents a word in itself or as part of a complex of signs that represents a single word is written in capital letters and with a 's' key</li>\n",
    "    <li>Numerical - A sign representing a number (or personal name determinative) has an extra key called 'sexified'. This gives information on the number sign's wedge structure.</li>\n",
    "</ol>\n",
    "\n",
    "In addition, a modified sign can be any of the three types above, but written with a nonstandard paleography (e.g. a diagonal wedge is incised in the clay instead of a horizontal). These are the signs we want to examine. They have extra data given under the 'mods' key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_signs(sign_data):\n",
    "    sign_info = {}\n",
    "    if 'v' in sign_data:\n",
    "        sign_info['b'] = sign_data['v']\n",
    "    if 's' in sign_data:\n",
    "        sign_info['b'] = sign_data['s']\n",
    "    if 'n' in sign_data:\n",
    "        sign_info['b'] = sign_data.get('sexified',sign_data.get('form','noform?'))\n",
    "    if 'mods' in sign_data:\n",
    "        for m in sign_data['mods']:\n",
    "            for d in m:\n",
    "                sign_info[d] = m[d]\n",
    "    if 'break' in sign_data:\n",
    "        sign_info['break'] = sign_data['break']\n",
    "    sign_info['sign_loc_id'] = sign_data.get('id','no-id')\n",
    "    return sign_info    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = set()\n",
    "all_signs = []\n",
    "for fname in os.listdir('corpusjson'):\n",
    "    #if not fname == 'P224485.json':\n",
    "    #    continue\n",
    "    f = codecs.open('corpusjson/'+fname,'r','utf-8')\n",
    "    j = json.load(f)\n",
    "    #print(str(len(j['cdl'][0]['cdl'])))\n",
    "    #print(fname)\n",
    "    for a in j['cdl'][0]['cdl']:\n",
    "        if a.get('type','') == 'discourse':\n",
    "            for b in a['cdl']:\n",
    "                if b.get('type','') == 'sentence':\n",
    "                    line_label = ''                    \n",
    "                    for c in b['cdl']:\n",
    "                        if c.get('node','') == 'd': #This is the label for the line e.g. \"o ii 3\"\n",
    "                            line_label = c.get('label','nolabel')\n",
    "                        if c.get('node','') == 'l':\n",
    "                            form = c['f']['form']\n",
    "                            for sign_data in c['f']['gdl']:\n",
    "                                if sign_data.get('det','') == 'semantic':\n",
    "                                    for sd in sign_data['seq']:\n",
    "                                        sign_info = process_signs(sd)\n",
    "                                        sign_info.update({'file':fname,'line_label':line_label,'form': form})\n",
    "                                        all_signs.append(sign_info)\n",
    "                                else:\n",
    "                                    sign_info = process_signs(sign_data)\n",
    "                                    sign_info.update({'file':fname,'line_label':line_label,'form': form})\n",
    "                                    all_signs.append(sign_info)\n",
    "                        if c.get('node','') == 'c':\n",
    "                            for d in c['cdl']:\n",
    "                                if d.get('node','') == 'l':\n",
    "                                    form = d['f']['form']\n",
    "                                    for sign_data in d['f']['gdl']:\n",
    "                                        if sign_data.get('det','') == 'semantic':\n",
    "                                            for sd in sign_data['seq']:\n",
    "                                                sign_info = process_signs(sd)\n",
    "                                                sign_info.update({'file':fname,'line_label':line_label,'form': form})\n",
    "                                                all_signs.append(sign_info)\n",
    "                                        else:\n",
    "                                            sign_info = process_signs(sign_data)\n",
    "                                            sign_info.update({'file':fname,'line_label':line_label,'form': form})\n",
    "                                            all_signs.append(sign_info)\n",
    "                        #types.add(c.get('type','no type'))\n",
    "                        \n",
    "all_signs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we form our Data Frame where each row contains information on every sign in the corpus. Further limitations on which signs are significant to our purposes will be made later, but for now we will eliminate all of the signs which are labelled as \"missing,\" (i.e. reconstructed) because any information based on their paleography or orthography cannot be ascertained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(all_signs)\n",
    "df = df.fillna('')\n",
    "df = df[(df['break'] != 'missing')]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1. Setting Up the Data for Clustering</h3>\n",
    "<p>The general goal is to assign a vector to each text that reflects the usage of variant orthography and paleography.</p>\n",
    "<ol>\n",
    "    <li>Paleography - Any one set of wedges that we classify as a sign can be impressed on the clay in different ways. For example, a wedge can be missing or one can be added. Also, the tilt of a wedge can variable. These are the features we want to examine in order to see if one text prefers one sign writing or another.</li>\n",
    "    <li>Orthography - Due to the homophony of the cuneiform writing system, one syllable can be written with many signs. For example, 'li' can be written with the LI-sign but also with the NI-sign, in which case it would be transliterated as li<sub>2</sub></li>\n",
    "</ol>\n",
    "<p>Other variables can be applied to a text as attributes in its vector. (What are these? We talked about things like Provenence, city information, scribe information. Also, if we apply different types of variables how can we use a clustering algorithm to treat these vector components as a different entity?).</p>\n",
    "<p>This section therefore contains two subsections. One groups the diagnostic signs with or without modifications per text to. The other discovers the homophonous signs used throughout the corpus and groups different usages per text</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, let's create more columns in the data frame to aid us\n",
    "<ol>\n",
    "<li>mods_str - Since the data contains three columns currently with information on variable paleography, it would help us to consolidate them into one column</li>\n",
    "<li>str_part and num_part - In order to determine which signs share a syllabic value, it will be useful to separate the transliterated readings into their string components and numerical components. Once we do this, we can group rows with the same str_part and count up the different usages of homophonous signs</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['sign_form'] = df['b'].apply(lambda x: sign_index.get(x.lower(),'?'))\n",
    "df['mods_str'] = df['a'] + '.' + df['f'] + '.' + df['m']\n",
    "\n",
    "import re\n",
    "def get_num_part(s):\n",
    "    try:\n",
    "        n = re.findall(r'[₀₁₂₃₄₅₆₇₈₉]+',s)[0]\n",
    "        n = n.replace('₀','0').replace('₁','1').replace('₂','2').replace('₃','3').replace('₄','4')\n",
    "        n = n.replace('₅','5').replace('₆','6').replace('₇','7').replace('₈','8').replace('₉','9')\n",
    "    except:\n",
    "        n = 1\n",
    "    return n\n",
    "def get_str_part(s):\n",
    "    try:\n",
    "        n = re.findall(r'[a-zA-ZšŠṣṢṭṬʾ \\(\\)0-9]+',s)[0]\n",
    "    except:\n",
    "        n = s\n",
    "    return n\n",
    "        \n",
    "df['str_part'] = df['b'].apply(lambda x: get_str_part(x))\n",
    "df['num_part'] = df['b'].apply(lambda x: get_num_part(x))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1.1: Collection of Modified Signs</h3>\n",
    "<p>The Data Frame we have contains the entire collection of signs in the corpus. However, not every sign has variants in paleography (at least according to Parpola's data input). We only want to look at the signs which have these variants, which we will term diagnostic. In the data, they are the signs that include any type of modification</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[~(df['f'] == '') | ~(df['a'] == '') | ~(df['m'] == '')]\n",
    "mod_signs = sorted(list(df2['sign_form'].unique()))\n",
    "mod_signs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now limit our Data Frame to include ONLY these diagnostic signs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_modsigns = df[df['sign_form'].isin(mod_signs)]\n",
    "#remove damaged signs too\n",
    "df_modsigns = df_modsigns[df_modsigns['break'] != 'damaged']\n",
    "df_modsigns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>1.2: Collection of Homophonous Signs</h3>\n",
    "<p>We now limit the original data frame in different way based on orthography. First we need to figure out which syllabic readings have multiple signs that can render them.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(df.groupby(['str_part'])['num_part'].agg('nunique'))\n",
    "ortho_list = list(df2[df2[('num_part')] > 1].index)\n",
    "ortho_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to eliminate capital letter entries because indices on logograms indicate different words and are not relevant here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ortho_list = [h for h in ortho_list if len(re.findall(r'[A-Z]',h)) == 0]\n",
    "ortho_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limit the dataframe to only these signs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ortho_signs = df[df['str_part'].isin(ortho_list)]\n",
    "df_ortho_signs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2. Mixed vs. Complementary Distribution</h3>\n",
    "<p>One of the goals of this project is to determine a preference for sign usage in one subgroup of the corpus versus another. To that end there is one more factor that needs to be discussed, namely the usage of these paleographic or orthographic variants within context. If the usage of these variants are context-dependent, meaning that one form or syllable is used in one context and another form or syllable in another context, it does not tell us much about the preferential usage of the signs. This is known as a complementary distribution. For example, if a scribe uses <i>li<sub>2</sub></i> only in the form of the word be-li<sub>2</sub> and the <i>li</i> sign in all other contexts, the choice of sign usage is not determined by the scribe's preference rather on scribal convention. This convention would thus be utilized by every scribe of this corpus and not help us to detect subgroups among these texts where scribes differ.</p>\n",
    "<p>On the other hand, if sign form or syllable variants appear within the same contexts, it gives us the information we want on scribal writing preference or tendencies. For example, <i>ia</i> and <i>ia<sub>2</sub></i> both appear in forms of the word bēliya, meaning that a scribe had an option of orthography and incised one or the other. (NTS: I'm avoiding the term \"choose\" here because it is a very loaded term with implications that may be misleading here). The question then becomes whether certain texts group together based on their tendencies to use one variant within a mixed distribution versus another variant.</p>\n",
    "<p>(paragraph about this dichotomy on the paleographic side of things. Mention TA vs. TA@v)</p>\n",
    "<p>(closing paragraph summarizing the issue)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>2.1 Paleographic Variant Distribution</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mods_agg = pd.DataFrame(df_modsigns.groupby(['sign_form','form','mods_str'])['a'].agg('count')).reset_index()\n",
    "df_mods_agg.columns = ['sign_form','form','mods_str','count']\n",
    "#first let's remove where total instances are less than a certain arbitrary value, say 5\n",
    "df_mods_agg = df_mods_agg[df_mods_agg['count'] >= 5]\n",
    "#NOW find and only keep the rows where sign_form and form are duplicates\n",
    "df_mods_agg['is_dup'] = df_mods_agg.duplicated(['sign_form','form'],False)\n",
    "df_mods_agg = df_mods_agg[df_mods_agg['is_dup'] == True]\n",
    "df_mods_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_select_signmods = df_mods_agg[['sign_form','mods_str']].drop_duplicates()\n",
    "df_select_signmods['combined'] = df_select_signmods['sign_form'] + ':' + df_select_signmods['mods_str']\n",
    "df_select_signmods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_signs = list(df_mods_agg['sign_form'].unique())\n",
    "select_signs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_file_select_signs = df_modsigns[df_modsigns['sign_form'].isin(select_signs)]\n",
    "df_file_select_signs = pd.DataFrame(df_file_select_signs.groupby(['file','sign_form','mods_str'])['a'].agg('count')).reset_index()\n",
    "df_file_select_signs['combined'] = df_file_select_signs['sign_form'] + ':' + df_file_select_signs['mods_str']\n",
    "df_file_select_signs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_file_select_signs = {}\n",
    "file_names = df_modsigns['file'].unique()\n",
    "for f in file_names:\n",
    "    d = {}\n",
    "    e = {}\n",
    "    for i, row in df_select_signmods.iterrows():\n",
    "        try:\n",
    "            n = int(df_file_select_signs[(df_file_select_signs['file'] == f) & (df_file_select_signs['combined'] == row['combined'])]['a']) + 1\n",
    "        except TypeError:\n",
    "            n = 1\n",
    "            \n",
    "        d[row['combined']] = n\n",
    "        if row['sign_form'] in e:\n",
    "            e[row['sign_form']] += n\n",
    "        else:\n",
    "            e[row['sign_form']] = n\n",
    "        \n",
    "    #d_select_signs[f] = [d['ia'] / ia_tot,d['ia₂'] / ia_tot,d['li'] / li_tot,d['li₂'] / li_tot,d['ša'] / sa_tot, d['ša₂'] / sa_tot,d['šu'] / su_tot,d['šu₂'] / su_tot]\n",
    "    d_file_select_signs[f] = []\n",
    "    for i,row in df_select_signmods.iterrows():\n",
    "        d_file_select_signs[f].append(d[row['combined']] / e[row['sign_form']])\n",
    "df_file_select_signs_c = pd.DataFrame(d_file_select_signs).transpose()\n",
    "df_file_select_signs_c.columns = list(df_select_signmods['combined'])\n",
    "df_file_select_signs_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>2.1.2. Clustering on Paleography Alone</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "km1 = KMeans(n_clusters=14, max_iter=1000).fit(df_file_select_signs_c)\n",
    "\n",
    "labels_paleo = {}\n",
    "\n",
    "km1.labels_\n",
    "\n",
    "for i in range(len(km1.labels_)):\n",
    "    if km1.labels_[i] in labels_paleo:\n",
    "        labels_paleo[km1.labels_[i]].append(file_names[i])\n",
    "    else:\n",
    "        labels_paleo[km1.labels_[i]] = [file_names[i]]\n",
    "labels_paleo\n",
    "\n",
    "#Sennacherib the Prince\n",
    "sar = ['P334141.json','P334390.json']\n",
    "#Nabu-pašir, governor of Harran\n",
    "np = ['P334807.json','P334080.json']\n",
    "#Nabu-deʾiq\n",
    "nd = ['P334568.json','P334792.json']\n",
    "\n",
    "def find_cluster(pnum,labels):\n",
    "    for k in labels:\n",
    "        if pnum in labels[k]:\n",
    "            return str(k)\n",
    "        \n",
    "print('Sennacherib clusters are: ',find_cluster(sar[0],labels_paleo),' and ',find_cluster(sar[1],labels_paleo))\n",
    "print('Nabu-pašir clusters are: ',find_cluster(np[0],labels_paleo),' and ',find_cluster(np[1],labels_paleo))\n",
    "print('Nabu-deʾiq clusters are: ',find_cluster(nd[0],labels_paleo),' and ',find_cluster(nd[1],labels_paleo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>2.2. Orthographic Variant Distribution</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_ortho_signs['form_str_part'] = df_ortho_signs['form'].apply(lambda x: re.sub(r'[₁₂₃₄₅₆₇₈₉₀]','',x))\n",
    "df_ortho_signs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_syls_agg = pd.DataFrame(df_ortho_signs.groupby(['str_part','form_str_part','b'])['a'].agg('count')).reset_index()\n",
    "df_syls_agg.columns = ['str_part','form_str_part','b','count']\n",
    "#first let's remove where total instances are less than a certain arbitrary value, say 5\n",
    "df_syls_agg = df_syls_agg[df_syls_agg['count'] >= 5]\n",
    "#NOW find and only keep the rows where sign_form and form are duplicates\n",
    "df_syls_agg['is_dup'] = df_syls_agg.duplicated(['str_part','form_str_part'],False)\n",
    "df_syls_agg = df_syls_agg[df_syls_agg['is_dup'] == True]\n",
    "df_syls_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_select_bs = df_syls_agg[['str_part','b']].drop_duplicates()\n",
    "#Don't need to create combined column here because b is sufficient\n",
    "#df_select_signmods['combined'] = df_select_signmods['sign_form'] + ':' + df_select_signmods['mods_str']\n",
    "df_select_bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "select_syls = list(df_syls_agg['str_part'].unique())\n",
    "select_syls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_file_select_bs = df_ortho_signs[df_ortho_signs['str_part'].isin(select_syls)]\n",
    "df_file_select_bs = pd.DataFrame(df_file_select_bs.groupby(['file','str_part','b'])['a'].agg('count')).reset_index()\n",
    "#Again combined is just b\n",
    "#df_file_select_syls['combined'] = df_file_select_signs['sign_form'] + ':' + df_file_select_signs['mods_str']\n",
    "df_file_select_bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_file_select_syls = {}\n",
    "file_names = df_ortho_signs['file'].unique()\n",
    "for f in file_names:\n",
    "    d = {}\n",
    "    e = {}\n",
    "    for i, row in df_select_bs.iterrows():\n",
    "        try:\n",
    "            n = int(df_file_select_bs[(df_file_select_bs['file'] == f) & (df_file_select_bs['b'] == row['b'])]['a']) + 1\n",
    "        except TypeError:\n",
    "            n = 1\n",
    "            \n",
    "        d[row['b']] = n\n",
    "        if row['str_part'] in e:\n",
    "            e[row['str_part']] += n\n",
    "        else:\n",
    "            e[row['str_part']] = n\n",
    "        \n",
    "    #d_select_signs[f] = [d['ia'] / ia_tot,d['ia₂'] / ia_tot,d['li'] / li_tot,d['li₂'] / li_tot,d['ša'] / sa_tot, d['ša₂'] / sa_tot,d['šu'] / su_tot,d['šu₂'] / su_tot]\n",
    "    d_file_select_syls[f] = []\n",
    "    for i,row in df_select_bs.iterrows():\n",
    "        d_file_select_syls[f].append(d[row['b']] / e[row['str_part']])\n",
    "df_file_select_syls_c = pd.DataFrame(d_file_select_syls).transpose()\n",
    "df_file_select_syls_c.columns = list(df_select_bs['b'])\n",
    "df_file_select_syls_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>2.2.2 Cluster using K-Means</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "km1 = KMeans(n_clusters=7, max_iter=1000).fit(df_file_select_syls_c)\n",
    "\n",
    "labels_ortho = {}\n",
    "\n",
    "km1.labels_\n",
    "\n",
    "for i in range(len(km1.labels_)):\n",
    "    if km1.labels_[i] in labels_ortho:\n",
    "        labels_ortho[km1.labels_[i]].append(file_names[i])\n",
    "    else:\n",
    "        labels_ortho[km1.labels_[i]] = [file_names[i]]\n",
    "labels_ortho\n",
    "\n",
    "#Let's examine some test cases. We'll select three pairs of texts, which we would expect to cluster always in the same way.\n",
    "#Sennacherib the Prince\n",
    "sar = ['P334141.json','P334390.json']\n",
    "#Nabu-pašir, governor of Harran\n",
    "np = ['P334807.json','P334080.json']\n",
    "#Nabu-deʾiq\n",
    "nd = ['P334568.json','P334792.json']\n",
    "\n",
    "def find_cluster(pnum,labels):\n",
    "    for k in labels:\n",
    "        if pnum in labels[k]:\n",
    "            return str(k)\n",
    "\n",
    "print('Sennacherib clusters are: ',find_cluster(sar[0],labels_ortho),' and ',find_cluster(sar[1],labels_ortho))\n",
    "print('Nabu-pašir clusters are: ',find_cluster(np[0],labels_ortho),' and ',find_cluster(np[1],labels_ortho))\n",
    "print('Nabu-deʾiq clusters are: ',find_cluster(nd[0],labels_ortho),' and ',find_cluster(nd[1],labels_ortho))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine Orthography and Paleography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tm_all = pd.concat([df_file_select_syls_c,df_file_select_signs_c],axis=1)\n",
    "tm_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "km1 = KMeans(n_clusters=14, max_iter=1000).fit(tm_all)\n",
    "\n",
    "labels_all = {}\n",
    "\n",
    "km1.labels_\n",
    "\n",
    "for i in range(len(km1.labels_)):\n",
    "    if km1.labels_[i] in labels_all:\n",
    "        labels_all[km1.labels_[i]].append(file_names[i])\n",
    "    else:\n",
    "        labels_all[km1.labels_[i]] = [file_names[i]]\n",
    "labels_all\n",
    "\n",
    "#Sennacherib the Prince\n",
    "sar = ['P334141.json','P334390.json']\n",
    "#Nabu-pašir, governor of Harran\n",
    "np = ['P334807.json','P334080.json']\n",
    "#Nabu-deʾiq\n",
    "nd = ['P334568.json','P334792.json']\n",
    "\n",
    "def find_cluster(pnum,labels):\n",
    "    for k in labels:\n",
    "        if pnum in labels[k]:\n",
    "            return str(k)\n",
    "\n",
    "print('Sennacherib clusters are: ',find_cluster(sar[0],labels_all),' and ',find_cluster(sar[1],labels_all))\n",
    "print('Nabu-pašir clusters are: ',find_cluster(np[0],labels_all),' and ',find_cluster(np[1],labels_all))\n",
    "print('Nabu-deʾiq clusters are: ',find_cluster(nd[0],labels_all),' and ',find_cluster(nd[1],labels_all))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>So it appears that letters from the same place do not group in the same clusters according to paleographic and orthographic preferences in the letters. Why should this be? Here are some options</p>\n",
    "<ol>\n",
    "    <li>Scribal usage of different paleographies and orthographies is not based on a certain preference either consiously or unconsiously. In other words, for any given scribe, free variation reigns supreme (expand on this)</li>\n",
    "    <li>Paleographic and Orthographic variation CAN indicate scribal tendencies, BUT computational methods are insufficient to determine this because machine learning algorithms require large amounts of data and the letters simply do not provide enough data</li>\n",
    "    <li>There is a problem with my methodology. Maybe I set up the text vectors incorrectly. Maybe I should include more orthographies/paleographies or perhaps less. Maybe the number of clusters selected is wrong.</li>\n",
    "</ol>\n",
    "\n",
    "<p>Something else to keep in mind here is that while I limited the number of signs to be considered in the text vectors, I did not restrict any text from being in the corpus. Perhaps I should do that. Maybe certain texts are simply too short to make any determinations on its grouping among the other texts.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize with MDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import MDS\n",
    "\n",
    "texts_2d_map = {}\n",
    "texts = tm_all.index\n",
    "\n",
    "mds1 = MDS(n_components = 2)\n",
    "texts_2d = mds1.fit_transform(tm_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_list = ['white','yellow','green','red','blue','brown','black']\n",
    "colors_all = []\n",
    "for i in range(len(km1.labels_)):\n",
    "    colors_all.append(color_list[km1.labels_[i]])\n",
    "colors_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(num=None, figsize=(16, 16), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "x_values = [xy[0] for xy in texts_2d]\n",
    "y_values = [xy[1] for xy in texts_2d]\n",
    "plt.scatter(x_values,y_values,c=colors_all)\n",
    "for i in range(len(texts_2d)):\n",
    "    plt.annotate(texts[i],(x_values[i],y_values[i]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THE FOLLOWING CONTAINS OLDER CODE from different attempts at forming text vectors. But I'm keeping it for now in case I need it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 Syls Material"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Now let's try to apply a quantitative method to figure out the level of mixed distribution which orthographic variants bear within word forms. The steps here are:\n",
    "<ol>\n",
    "<li>Create a dictionary that hashes each syllable to a list of its orthographic variants</li>\n",
    "<li>Count up the number of instances of one orthographic variant being used in each word form</li>\n",
    "<li>Match up the word forms with either variant and see the total numbers of that one word form with both variants together</li>\n",
    "</ol>\n",
    "\n",
    "df_agg = pd.DataFrame(df_ortho_signs.groupby(['str_part','b'])['a'].agg('count')).reset_index()\n",
    "df_agg.columns = ['str_part','b','count']\n",
    "ortho_list_tuples = {}\n",
    "for i, row in df_agg.iterrows():\n",
    "    if row['str_part'] in ortho_list_tuples:\n",
    "        ortho_list_tuples[row['str_part']].append(row['b'])\n",
    "    else:\n",
    "        ortho_list_tuples[row['str_part']] = [row['b']]\n",
    "\n",
    "#clean up the u and ṭe directly. It is easier\n",
    "ortho_list_tuples['u1'] = ['u','u₂']\n",
    "ortho_list_tuples['u2'] = ['u','u₃']\n",
    "ortho_list_tuples['u3'] = ['u₂','u₃']\n",
    "ortho_list_tuples['ṭe1'] = ['ṭe','ṭe₂']\n",
    "ortho_list_tuples['ṭe2'] = ['ṭe','ṭe₃']\n",
    "ortho_list_tuples['ṭe3'] = ['ṭe₂','ṭe₃']\n",
    "ortho_list_tuples\n",
    "\n",
    "df_form_counts = pd.DataFrame(df_ortho_signs.groupby(['str_part','b','form'])['a'].agg('count'))\n",
    "df_form_counts = df_form_counts.reset_index()\n",
    "df_form_counts = df_form_counts.sort_values(by=['b','a'],ascending=[True,False])\n",
    "df_form_counts\n",
    "\n",
    "l_mixed = []\n",
    "for k in ortho_list_tuples:\n",
    "    if(len(ortho_list_tuples[k]) == 2):\n",
    "        df_ortho1 = df_form_counts[df_form_counts['b'] == ortho_list_tuples[k][0]]\n",
    "        df_ortho2 = df_form_counts[df_form_counts['b'] == ortho_list_tuples[k][1]]\n",
    "        total_count = 0\n",
    "        mixed_count = 0\n",
    "        for i, row1 in df_ortho1.iterrows():\n",
    "            form1 = re.sub(r'' + k + '[₁₂₃₄₅₆₇₈₉₀]+',k,row1['form'])\n",
    "            for j, row2 in df_ortho2.iterrows():\n",
    "                total_count += 1\n",
    "                form2 = re.sub(r'' + k + '[₁₂₃₄₅₆₇₈₉₀]+',k,row2['form'])\n",
    "                if form1 == form2:\n",
    "                    data = {\n",
    "                        'str_part': k,\n",
    "                        'form1': row1['form'],\n",
    "                        'form1_c': row1['a'],\n",
    "                        'form2': row2['form'],\n",
    "                        'form2_c': row2['a'],\n",
    "                        'form_base': form1,\n",
    "                    }\n",
    "                    l_mixed.append(data)\n",
    "\n",
    "df_mixed = pd.DataFrame(l_mixed)\n",
    "df_mixed['total_mixed'] = df_mixed['form1_c'] * df_mixed['form2_c']\n",
    "df_mixed\n",
    "\n",
    "This is the chart to look at to see which orthographic variants are being employed in a meaningful mixed distribution.\n",
    "<p>(A note here about perhaps having a cutoff point for total instances for one syllable)</p>\n",
    "<p>(A note here about the important orthographic variation included in this chart and how to employ it in our text vector. Do we want to look at the variants only within the word forms with sufficient number of instances or across the whole text? I would say across the whole text. Do we want to restrict the dimensions of the text vectors to only orthographic variants that we select from the chart or employ a kind of weighting system which places more importance on the syllables that are important here? For now, I'm just going to limit it to particular syllables)</p>\n",
    "<p>(ALSO, do we want to match up orthographic variants in the context of word forms OR in the context of 2-grams, per David's suggestion. I did entire word forms here, because I found the programming to be easier.)</p>\n",
    "\n",
    "df_4_syls = df_ortho_signs[df_ortho_signs['str_part'].isin(['šu','ša','ia','li'])]\n",
    "df_4_syls = pd.DataFrame(df_4_syls.groupby(['file','str_part','b'])['a'].agg('count')).reset_index()\n",
    "df_4_syls\n",
    "\n",
    "d_file_4_syls = {}\n",
    "file_names = df_ortho_signs['file'].unique()\n",
    "for f in file_names:\n",
    "    d = {}\n",
    "    for s in ['ia','ia₂','šu','šu₂','ša','ša₂','li','li₂']:\n",
    "        try:\n",
    "            n = int(df_4_syls[(df_4_syls['file'] == f) & (df_4_syls['b'] == s)]['a']) + 1\n",
    "        except TypeError:\n",
    "            n = 1\n",
    "        d[s] = n\n",
    "    \n",
    "    ia_tot = d['ia'] + d['ia₂']\n",
    "    su_tot = d['šu'] + d['šu₂']\n",
    "    sa_tot = d['ša'] + d['ša₂']\n",
    "    li_tot = d['li'] + d['li₂']\n",
    "    \n",
    "    d_file_4_syls[f] = [d['ia'] / ia_tot,d['ia₂'] / ia_tot,d['li'] / li_tot,d['li₂'] / li_tot,d['ša'] / sa_tot, d['ša₂'] / sa_tot,d['šu'] / su_tot,d['šu₂'] / su_tot]\n",
    "df_file_4_syls = pd.DataFrame(d_file_4_syls).transpose()\n",
    "df_file_4_syls.columns = ['ia','ia₂','li','li₂','ša','ša₂','šu','šu₂']\n",
    "df_file_4_syls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>2. Clustering</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2.1 Tf-Idf</h3>\n",
    "<p>We want to gather all of the signs with their variant orthographies into each text file and generate a vector which will contain a '1' if the text contains an orthography or sign value and a '0' if it does not.</p>\n",
    "<p>(Here's where I am unsure of how to form the Tf-Idf matrix. The code currently counts ALL usages of the signs per file. But do we want to...</p>\n",
    "<ol><li>Only give each sign usage a 1 or 0 value?</li><li>Keep the totals and normalize the vectors?</li></ol>\n",
    "<p>Another thing to consider is if we want to combine Tf-Idf matrices, i.e. the sign orthography variants and the sign syllable variants. Would this be useful? Would we need to distinguish these qualities in the vectors somehow? The same issue occurs if we tack on other variables to the text)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>2.1.1. Modified Signs Tf-Idf</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_file_modsigns = pd.DataFrame(df_modsigns.groupby(['file']).apply(lambda x: ' '.join(x.sign_form+':'+x.mods_str))).reset_index()\n",
    "df_file_modsigns.columns = ['file','mod_signs_all']\n",
    "df_file_modsigns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "trans = TfidfTransformer(smooth_idf=False)\n",
    "vect = CountVectorizer(token_pattern='[^ ]+',lowercase=False)\n",
    "vect_fit = vect.fit_transform(df_file_modsigns['mod_signs_all'])\n",
    "tfidf_fit = trans.fit_transform(vect_fit)\n",
    "tfidf1 = pd.DataFrame(tfidf_fit.toarray(),columns=vect.get_feature_names(),index=df_file_modsigns.file)\n",
    "tfidf1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>2.1.2. TF-IDF for homophonous signs</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_file_orthosigns = pd.DataFrame(df_ortho_signs.groupby(['file']).apply(lambda x: ' '.join(x.b))).reset_index()\n",
    "df_file_orthosigns.columns = ['file','homo_signs_all']\n",
    "df_file_orthosigns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect2 = CountVectorizer(token_pattern='[^ ]+',lowercase=False)\n",
    "vect_fit2 = vect2.fit_transform(df_file_orthosigns['homo_signs_all'])\n",
    "tfidf2 = pd.DataFrame(vect_fit2.toarray(),columns=vect2.get_feature_names(),index=df_file_orthosigns.file)\n",
    "tfidf2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2.2 K-Means Clustering</h3>\n",
    "<p>Now we use K-means to cluster the texts. (K-means requires a determination of how many clusters to use. What is this number? How should we determine it?)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>2.2.1. Clustering for Modified Signs</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "km1 = KMeans(n_clusters=7, max_iter=1000).fit(tfidf1)\n",
    "\n",
    "labels_mods = {}\n",
    "\n",
    "for i in range(len(km1.labels_)):\n",
    "    if km1.labels_[i] in labels_mods:\n",
    "        labels_mods[km1.labels_[i]].append(df_file_modsigns.file[i])\n",
    "    else:\n",
    "        labels_mods[km1.labels_[i]] = [df_file_modsigns.file[i]]\n",
    "labels_mods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same for homophone signs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>2.2.2. Clustering for Homophonous Signs</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km2 = KMeans(n_clusters=7, max_iter=1000).fit(tfidf2)\n",
    "labels_orthos = {}\n",
    "\n",
    "for i in range(len(km2.labels_)):\n",
    "    if km2.labels_[i] in labels_homos:\n",
    "        labels_orthos[km2.labels_[i]].append(df_file_orthosigns.file[i])\n",
    "    else:\n",
    "        labels_orthos[km2.labels_[i]] = [df_file_orthosigns.file[i]]\n",
    "labels_orthos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>2. Visualization</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>2.1. Visualize from Modified Sign Clusters</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import MDS\n",
    "\n",
    "texts_2d_map = {}\n",
    "texts = tfidf1.index\n",
    "\n",
    "dim_num = len(vect.get_feature_names())\n",
    "mds1 = MDS(n_components = 2)\n",
    "texts_2d = mds1.fit_transform(tfidf1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up colors for each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_list = ['white','yellow','green','red','blue','brown','black']\n",
    "colors_all = []\n",
    "for i in range(len(km1.labels_)):\n",
    "    colors_all.append(color_list[km1.labels_[i]])\n",
    "colors_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(num=None, figsize=(16, 16), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "x_values = [xy[0] for xy in texts_2d]\n",
    "y_values = [xy[1] for xy in texts_2d]\n",
    "plt.scatter(x_values,y_values,c=colors_all)\n",
    "for i in range(len(texts_2d)):\n",
    "    plt.annotate(texts[i],(x_values[i],y_values[i]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>2.2 Same for homophone signs</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_num = len(vect2.get_feature_names())\n",
    "mds2 = MDS(n_components = 2)\n",
    "texts_2d = mds2.fit_transform(tfidf2)\n",
    "\n",
    "x_values = [xy[0] for xy in texts_2d]\n",
    "y_values = [xy[1] for xy in texts_2d]\n",
    "plt.scatter(x_values,y_values,c=colors_all)\n",
    "for i in range(len(texts_2d)):\n",
    "    plt.annotate(texts[i],(x_values[i],y_values[i]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
